{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Notebook 1\n",
    "\n",
    "### Group 17\n",
    "\n",
    "NoÃ© HIRSCHAUER - id 105716\n",
    "\n",
    "Hugo KERINO - id 105478\n",
    "\n",
    "\n",
    "\n",
    "Use this notebook to reproduce the results of best prediction 1 (public score : 153.264505)\n",
    "\n",
    "#### Libraries and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import warnings\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from joblib import dump,load\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "starting_directory = 'C:/Users/NOE/source/repos/Energy-forecasting-NTNU/data/' # set to the directory that contains the A,B,C folders\n",
    "os.chdir(starting_directory)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "RANDOM_SEED = 59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Functions\n",
    "\n",
    "This section contains all the functions that are used for further data analysis/processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_location(location: str):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Read data from Parquet files located in a specified directory.\n",
    "\n",
    "    This function reads the training target, observed training data, estimated training data,\n",
    "    and estimated test data from Parquet files located in the specified directory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - `location` (str): The directory path where the Parquet files are located.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - A tuple containing the following DataFrames:\n",
    "      - `train_target` (pandas.DataFrame): Training target data.\n",
    "      - `X_train_observed` (pandas.DataFrame): Observed training data.\n",
    "      - `X_train_estimated` (pandas.DataFrame): Estimated training data.\n",
    "      - `X_test_estimated` (pandas.DataFrame): Estimated test data.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```python\n",
    "    data_location = '/path/to/data_directory'\n",
    "    train_target, X_train_observed, X_train_estimated, X_test_estimated = read_data_location(data_location)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    train_target = pd.read_parquet(location+'/train_targets.parquet')\n",
    "    X_train_observed = pd.read_parquet(location+'/X_train_observed.parquet')\n",
    "    X_train_estimated = pd.read_parquet(location+'/X_train_estimated.parquet')\n",
    "    X_test_estimated = pd.read_parquet(location+'/X_test_estimated.parquet')\n",
    "    return(train_target, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "\n",
    "\n",
    "def find_low_correlation_columns(dataframe, threshold, verbose = True):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Find and remove columns in a DataFrame with low absolute correlation coefficients.\n",
    "\n",
    "    This function calculates the pairwise correlation coefficients between columns in a DataFrame and\n",
    "    removes columns that have absolute correlation coefficients equal to or above the specified threshold.\n",
    "    It provides the option to print the removed columns for reference.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - `dataframe` (pandas.DataFrame): The DataFrame for which low correlation columns should be found.\n",
    "    - `threshold` (float): The threshold for absolute correlation coefficients. Columns with absolute\n",
    "                      correlation coefficients greater than or equal to this threshold will be removed.\n",
    "    - `verbose` (bool, optional): If True, print information about removed columns. Default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - List of column names that remain in the DataFrame after removing low correlation columns.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "\n",
    "    data = {'feature1': [1, 2, 3, 4, 5],\n",
    "            'feature2': [2, 3, 4, 5, 6],\n",
    "            'feature3': [5, 6, 7, 8, 9]}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    threshold = 0.7\n",
    "\n",
    "    selected_columns = find_low_correlation_columns(df, threshold)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    correlation_matrix = dataframe[:].corr()\n",
    "\n",
    "    feature_list = list(correlation_matrix.columns)\n",
    "    removed_list = []\n",
    "    for reference_index in range(len(feature_list)):\n",
    "        reference_feature = feature_list[reference_index]\n",
    "\n",
    "        # Check if this feature has not already been removed\n",
    "        if reference_feature not in removed_list:\n",
    "            for compared_index in range(reference_index+1,len(feature_list)):\n",
    "                compared_feature = feature_list[compared_index]\n",
    "\n",
    "                # Check if this feature has not already been removed\n",
    "                if compared_feature not in removed_list:\n",
    "\n",
    "                    # Get correlation coefficient\n",
    "                    coefficient = correlation_matrix.loc[reference_feature,compared_feature]\n",
    "\n",
    "                    # Remove column if it's a NaN\n",
    "                    if pd.isna(coefficient):\n",
    "                        removed_list.append(compared_feature)\n",
    "                        if verbose : print(f\"Removed {compared_feature} because it is NaN\")\n",
    "                        \n",
    "                    # Remove if it is above threshold\n",
    "                    if np.abs(coefficient)>=threshold:\n",
    "                        removed_list.append(compared_feature)\n",
    "                        if verbose : print(f\"Removed \\t{compared_feature}\\tbecause it is correlated to\\t{reference_feature}\")\n",
    "    \n",
    "    \n",
    "    output_list = feature_list.copy()\n",
    "    for element in removed_list:\n",
    "        output_list.remove(element)\n",
    "        \n",
    "    return list(output_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fill_nan(Dataframe: pd.DataFrame, feature: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Fill missing (`NaN`) values in a specified feature of a DataFrame using forward-fill or backward-fill.\n",
    "\n",
    "    This function checks for missing values in the specified feature and fills them using forward-fill (`ffill`)\n",
    "    or backward-fill (`bfill`) if necessary. It also provides information about the filled method and missing data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - `Dataframe` (pandas.DataFrame): The DataFrame containing the feature with missing values.\n",
    "    - `feature` (str): The name of the feature in the DataFrame.\n",
    "    - `verbose` (bool): whether to display information about the column being filled and the method used\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - None, but it updates the input DataFrame in place by filling missing values.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "\n",
    "    data = {'date': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n",
    "            'value': [1, 2, np.nan, 4, np.nan, 6, 7, 8, np.nan, 10]}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    feature = 'value'\n",
    "\n",
    "    fill_nan(df, feature)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    it_na = np.where(Dataframe[feature].isna())[0]\n",
    "    \n",
    "    #If missing value\n",
    "    if len(it_na != 0):\n",
    "        if verbose:\n",
    "            print(\"Missing data in\",feature,\" size \",it_na.size,\" out of\", Dataframe.shape[0])\n",
    "        new_column = Dataframe[feature].ffill()\n",
    "        method = 'ffill'\n",
    "        \n",
    "        if((np.any(new_column.isna()) == True)): \n",
    "            method = 'bfill'\n",
    "            new_column = Dataframe[feature].bfill()\n",
    "    \n",
    "        if (np.any(new_column.isna()) == False):\n",
    "            if verbose:\n",
    "                print(\"Filled with\",method,\"method\")\n",
    "            Dataframe[feature] = new_column\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"WARNING : still NaN values in \"+feature)\n",
    "\n",
    "def load_clean_datasets(location: str, threshold: float,verbose=True):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Load and preprocess datasets, handling missing values and highly correlated features.\n",
    "\n",
    "    This function loads four datasets from the specified location and processes them. It replaces colons in column names,\n",
    "    identifies low-correlation features, and fills missing values using the `fill_nan` function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - `location` (str): The path or URL to the data files.\n",
    "    - `threshold` (float): The threshold for feature correlation identification.\n",
    "    - `verbose` (bool): whether to display information about the dataset being processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - Tuple of DataFrames: A tuple containing four DataFrames for train_target, X_train_observed, X_train_estimated, and X_test_estimated.\n",
    "    - List of Lists of str: A list of lists containing the names of low-correlation features for each DataFrame.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```python\n",
    "    train_target, X_train_observed, X_train_estimated, X_test_estimated, FEATURES = load_clean_datasets(\"data/location\", 0.7)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    train_target, X_train_observed, X_train_estimated, X_test_estimated = read_data_location(location = location)\n",
    "\n",
    "\n",
    "    # Process NaNs and highly correlated features\n",
    "    dataframes = [train_target,X_train_observed,X_train_estimated,X_test_estimated]\n",
    "    dataframe_names = [\"train_target\",\"X_train_observed\",\"X_train_estimated\",\"X_test_estimated\"]\n",
    "    index = 0\n",
    "    FEATURES = [] # Contains the list of useful features for each dataframe\n",
    "\n",
    "\n",
    "    for dataframe in dataframes:\n",
    "        if verbose:\n",
    "            print(\"** WORKING ON \"+dataframe_names[index]+\"**\")\n",
    "        # Replace colons with underscores in column names of 'dataframe'\n",
    "        dataframe.columns = dataframe.columns.str.replace(':', '_')\n",
    "        # Find low find_low_correlation_columns\n",
    "        features = find_low_correlation_columns(dataframe,threshold,verbose=False)\n",
    "        FEATURES.append(features)\n",
    "        # Fill NaN\n",
    "        for feature in dataframe.columns:\n",
    "            fill_nan(dataframe,feature,verbose=False)\n",
    "        index+=1\n",
    "    return (train_target, X_train_observed, X_train_estimated, X_test_estimated,FEATURES)\n",
    "\n",
    "def select_and_resample(Xdf: pd.DataFrame,Ydf: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Select and resample time series data.\n",
    "\n",
    "    This function selects data from two time series dataframes, `Xdf` and `Ydf`, where there is a match in time values.\n",
    "    It then resamples `Ydf` to a 15-minute interval using linear interpolation and returns the resulting time-aligned data.\n",
    "    The output data contains only streaks of minutes `[15, 30, 45, 00]`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - `Xdf` (pandas.DataFrame): The first time series dataframe containing `'date_forecast'` column.\n",
    "    - `Ydf` (pandas.DataFrame): The second time series dataframe containing `'time'` column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - Tuple of two pandas DataFrames:\n",
    "      - The selected and filtered X values, with a 15-minute interval.\n",
    "      - The resampled Y values, matching the selected X values.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```python\n",
    "    Xdf = pd.DataFrame({'date_forecast': pd.date_range(start='2023-01-01', periods=100, freq='15T')})\n",
    "    Ydf = pd.DataFrame({'time': pd.date_range start='2023-01-01', periods=200, freq='30T')})\n",
    "    selected_X, selected_Y = select_and_resample(Xdf, Ydf)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    ##### Select Y values where there are X values :\n",
    "    Y_in_X_times = Ydf['time'].isin(Xdf['date_forecast'])\n",
    "    Selected_Y = Ydf[Y_in_X_times].reset_index(drop=True)\n",
    "    \n",
    "    ##### Select X values where there are Y values\n",
    "    # ceil the date forecast to the hour in Xdf\n",
    "    Xdf['hourly_forecast'] = Xdf['date_forecast'].dt.ceil('H')\n",
    "    X_in_Y_times = Xdf['hourly_forecast'].isin(Selected_Y['time'])\n",
    "    Selected_X = Xdf[X_in_Y_times].reset_index(drop=True).drop(columns=['hourly_forecast'])\n",
    "\n",
    "    #### Filter X values to get 4 consecutive 15 minutes\n",
    "    valid_minutes = [15, 30, 45, 0]\n",
    "    # Calculate the difference in minutes between consecutive rows\n",
    "    Selected_X = Selected_X.sort_values('date_forecast')\n",
    "    Selected_X['minute_diff'] = Selected_X['date_forecast'].diff().dt.total_seconds() / 60\n",
    "    # Create a mask to identify valid minutes\n",
    "    valid_mask = Selected_X['minute_diff'].isin(valid_minutes)\n",
    "    # Create a mask to identify the start of consecutive valid minute streaks\n",
    "    streak_start_mask = valid_mask & (~valid_mask.shift(1, fill_value=False))\n",
    "    # Create a mask to identify the end of consecutive valid minute streaks\n",
    "    streak_end_mask = valid_mask & (~valid_mask.shift(-1, fill_value=False))\n",
    "    # Merge the start and end masks to get the full streak mask\n",
    "    consecutive_streak_mask = streak_start_mask | streak_end_mask\n",
    "    # Extend the streaks to include isolated values\n",
    "    consecutive_streak_mask = consecutive_streak_mask | consecutive_streak_mask.shift(1, fill_value=False) | consecutive_streak_mask.shift(-1, fill_value=False)\n",
    "    # Repeatedly extend the streaks until no more can be extended\n",
    "    while True:\n",
    "        extended_streaks = consecutive_streak_mask | consecutive_streak_mask.shift(1, fill_value=False) | consecutive_streak_mask.shift(-1, fill_value=False)\n",
    "        if extended_streaks.equals(consecutive_streak_mask):\n",
    "            break\n",
    "        consecutive_streak_mask = extended_streaks\n",
    "    # Identify the cases where the streaks start with non-15 minutes\n",
    "    non_15_start = valid_mask & (streak_start_mask | streak_end_mask)\n",
    "    consecutive_streak_mask = consecutive_streak_mask | non_15_start\n",
    "\n",
    "    Selected_X_filtered = Selected_X[consecutive_streak_mask].drop(columns='minute_diff')\n",
    "\n",
    "    #### Resample Ydf with 15 minute interval \n",
    "    Selected_Y.set_index('time', inplace=True)\n",
    "    Selected_Y_resampled = Selected_Y.resample('15T').interpolate(method='linear') #  use 'nearest' for flatter interpolation\n",
    "    Selected_Y_resampled = Selected_Y_resampled.reset_index()\n",
    "\n",
    "    # Select Y resampled values where there are X values :\n",
    "    Y_in_X_times_resampled = Selected_Y_resampled['time'].isin(Selected_X_filtered['date_forecast'])\n",
    "    Selected_Y = Selected_Y_resampled[Y_in_X_times_resampled].reset_index(drop=True)\n",
    "    # Again the other way round to remove data that passed through the filter\n",
    "    Selected_X = Selected_X_filtered[Selected_X_filtered['date_forecast'].isin(Selected_Y['time'])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    return(Selected_X[1:], Selected_Y[1:])  # remove first row which always starts with a 00\n",
    "\n",
    "def remove_consecutive_identical_values(df: pd.DataFrame, feature: str):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Remove rows with consecutive identical values in a specified feature of a DataFrame.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    - `df` (pandas.DataFrame): The DataFrame to process.\n",
    "    - `feature` (str): The name of the feature in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - `pandas.DataFrame`: The filtered DataFrame with consecutive identical values removed.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "\n",
    "    data = {'time': pd.date_range(start='2023-01-01', periods=100, freq='15T'),\n",
    "            'value': [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    feature = 'value'\n",
    "\n",
    "    filtered_df = remove_consecutive_identical_values(df, feature)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Identify rows with consecutive identical values in the specified feature\n",
    "    mask = df[feature] != df[feature].shift(+1)\n",
    "\n",
    "    # Filter and remove rows with consecutive identical values\n",
    "    filtered_df = df[mask]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def select_o_clock(dataframe: pd.DataFrame, feature: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Select rows from a DataFrame where the time in the specified feature is a round hour (minute and second are 0) and sort them by the 'feature' column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - `dataframe` (pandas.DataFrame): The DataFrame to process.\n",
    "    - `feature` (str): The name of the feature column containing time values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - `pandas.DataFrame`: The filtered DataFrame with rows where the time is a round hour, sorted by the 'feature' column.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "\n",
    "    data = {'time_column': pd.date_range(start='2023-01-01', periods=100, freq='H')}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    feature = 'time_column'\n",
    "\n",
    "    filtered_df = select_o_clock(df, feature)\n",
    "    ```\n",
    "    \"\"\"\n",
    "     # Check if the feature column exists in the DataFrame\n",
    "    if feature not in dataframe.columns:\n",
    "        raise ValueError(f\"'{feature}' column not found in the DataFrame\")\n",
    "\n",
    "    # Check if the feature column is of datetime type\n",
    "    if not pd.api.types.is_datetime64_any_dtype(dataframe[feature]):\n",
    "        raise ValueError(f\"'{feature}' column should be of datetime type\")\n",
    "\n",
    "    # Filter the DataFrame to select rows with round-hour times\n",
    "    filtered_df = dataframe[dataframe[feature].dt.minute == 0 & (dataframe[feature].dt.second == 0)]\n",
    "    \n",
    "    # Sort the filtered DataFrame by the 'feature' column\n",
    "    filtered_df = filtered_df.sort_values(by=feature)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "\n",
    "def lagged_features_FUTURE(df, time_intervals = [15,30,45],verbose=False):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Generate lagged features in a DataFrame based on specified time intervals.\n",
    "\n",
    "    This function creates lagged features for selected columns in the DataFrame by shifting the data back in time based on\n",
    "    the provided time intervals. It then fills in missing values using the `fill_nan` function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - `df` (pandas.DataFrame): The DataFrame to which lagged features will be added.\n",
    "    - `time_intervals` (List[int]): List of time intervals (in minutes) for creating lagged features.\n",
    "    - `verbose` (bool): Whether to display information about the lagged features being created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - pandas.DataFrame: A new DataFrame with lagged features added and missing values filled.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```python\n",
    "    data = {'date_forecast': pd.date_range(start='2023-01-01', periods=100, freq='H'),\n",
    "            'value1': [1, 2, 3, 4, 5],\n",
    "            'value2': [10, 20, 30, 40, 50]}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    time_intervals = [15, 30, 45]\n",
    "\n",
    "    lagged_df = lagged_features(df, time_intervals)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    if time_intervals==0:\n",
    "        return df\n",
    "    \n",
    "    new_df = df.copy()      \n",
    "    new_columns = []\n",
    "\n",
    "    for interval in time_intervals:\n",
    "        # Create a suffix for the column names\n",
    "        suffix = '__p' + str(interval)\n",
    "\n",
    "        lagged_df = new_df.shift(-interval//15).copy()\n",
    "\n",
    "        for column in df.columns:\n",
    "            if column not in ['date_forecast', 'date_calc','sin_year','cos_year']:\n",
    "                lag_name=column + suffix\n",
    "                if verbose: print(lag_name)\n",
    "                # Add lagged column\n",
    "                new_column = pd.DataFrame()\n",
    "                new_column[lag_name] = new_df[column].copy()\n",
    "                # Get the value at the shifted time\n",
    "                mask = (new_df['date_forecast']+pd.to_timedelta(interval, unit='minutes') == lagged_df['date_forecast'])\n",
    "                new_column.loc[mask,lag_name] = lagged_df.loc[mask,column]\n",
    "                # Fix values where there is no data available\n",
    "                new_column.loc[np.logical_not(mask),lag_name]=np.nan\n",
    "                new_columns.append(new_column)\n",
    "\n",
    "    new_df = pd.concat([new_df]+new_columns,axis=1)\n",
    "\n",
    "    # Fill missing values with the last known value for each specific column\n",
    "    for column in new_df.columns:\n",
    "        fill_nan(new_df,column,verbose=False)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def normalize_dataframe_minmax(df: pd.DataFrame,verbose = False):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Normalize a DataFrame using min-max scaling, excluding datetime columns.\n",
    "\n",
    "    This function performs min-max scaling to normalize each column in the DataFrame between 0 and 1,\n",
    "    excluding datetime columns. It replaces the original values with the scaled values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - `df` (pandas.DataFrame): The DataFrame to be normalized.\n",
    "    - `verbose` (bool): Whether to display information about the columns being normalized.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - pandas.DataFrame: A new DataFrame with columns normalized between -1 and 1.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```python\n",
    "    data = {'feature1': [1, 2, 3, 4, 5],\n",
    "            'feature2': [10, 20, 30, 40, 50],\n",
    "            'date': pd.date_range(start='2023-01-01', periods=5, freq='D')}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    normalized_df = normalize_dataframe(df)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    normalized_df = df.copy()\n",
    "    # Define the range [0, 1]\n",
    "    min_range = 0\n",
    "    max_range = 1\n",
    "    \n",
    "    # Iterate over each column and apply min-max scaling, excluding datetime columns\n",
    "    for column in normalized_df.columns:\n",
    "        if verbose:\n",
    "            print(column)\n",
    "        if normalized_df[column].dtype not in [pd.Timestamp, pd.DatetimeTZDtype,'<M8[us]']:\n",
    "            min_val = normalized_df[column].min()\n",
    "            max_val = normalized_df[column].max()\n",
    "            if verbose:\n",
    "                print(f\"min:{min_val:.2f}\\tmax:{max_val:.2f}\")\n",
    "                print(f\"different:{min_val!=max_val}\")\n",
    "            # Normalize the non-datetime column between -1 and 1\n",
    "            if min_val!=max_val:\n",
    "                normalized_df[column] = min_range + (max_range - min_range) * (normalized_df[column] - min_val) / (max_val - min_val)\n",
    "            else :\n",
    "                normalized_df[column] = min_range + (max_range - min_range) * normalized_df[column]/max(max_val,1)\n",
    "\n",
    "    return normalized_df\n",
    "\n",
    "def normalize_dataframe_minmax_TEST(train_df: pd.DataFrame,test_df: pd.DataFrame,verbose = False):\n",
    "\n",
    "    normalized_train = train_df.copy()\n",
    "    normalized_test = test_df.copy()\n",
    "    # Define the range [0, 1]\n",
    "    min_range = 0 # best 0\n",
    "    max_range = 1\n",
    "    \n",
    "    # Iterate over each column and apply min-max scaling, excluding datetime columns\n",
    "    for column in train_df.columns:\n",
    "        if verbose:\n",
    "            print(column)\n",
    "        if train_df[column].dtype not in [pd.Timestamp, pd.DatetimeTZDtype,'<M8[us]']:\n",
    "            min_val = min(train_df[column].min(),test_df[column].min())\n",
    "            max_val = max(train_df[column].max(),test_df[column].max())\n",
    "            if verbose:\n",
    "                print(f\"min:{min_val:.2f}\\tmax:{max_val:.2f}\")\n",
    "                print(f\"different:{min_val!=max_val}\")\n",
    "            # Normalize the non-datetime column between -1 and 1\n",
    "            if min_val!=max_val:\n",
    "                normalized_train[column] = min_range + (max_range - min_range) * (normalized_train[column] - min_val) / (max_val - min_val)\n",
    "                normalized_test[column] = min_range + (max_range - min_range) * (normalized_test[column] - min_val) / (max_val - min_val)\n",
    "            else :\n",
    "                normalized_train[column] = min_range + (max_range - min_range) * normalized_train[column]/max(max_val,1)\n",
    "                normalized_test[column] = min_range + (max_range - min_range) * normalized_test[column]/max(max_val,1)\n",
    "\n",
    "    return (normalized_train, normalized_test)\n",
    "\n",
    "\n",
    "\n",
    "def convert_datetime(df: pd.DataFrame, time_feature: str):\n",
    "    new_df = df.copy()\n",
    "\n",
    "    # Convert the 'time_feature' column to datetime if it's not already\n",
    "    new_df[time_feature] = pd.to_datetime(new_df[time_feature])\n",
    "\n",
    "    # Calculate the number of seconds since midnight and subtract it from 86400 to make it -1 at midnight\n",
    "    new_df['seconds_since_midnight'] = (new_df[time_feature] - new_df[time_feature].dt.normalize()).dt.total_seconds()\n",
    "\n",
    "    # Calculate the sine of the day with -1 at midnight\n",
    "    new_df['sin_day'] = np.sin(2 * np.pi * new_df['seconds_since_midnight']/86400- np.pi/2)\n",
    "\n",
    "    # Calculate the sine of the year with -1 on January 1st\n",
    "    new_df['sin_year'] = np.sin(2 * np.pi * (new_df[time_feature].dt.dayofyear - 1)/365 - np.pi/2)\n",
    "\n",
    "    # Calculate the cosine of the year with -1 on January 1st\n",
    "    new_df['cos_year'] = np.cos(2 * np.pi * (new_df[time_feature].dt.dayofyear - 1)/365 - np.pi/2)\n",
    "\n",
    "    # Calculate the difference in seconds between 'time_feature' and 'date_calc'\n",
    "    new_df['forecast_interval'] = (new_df[time_feature] - new_df['date_calc']).dt.total_seconds().fillna(0)\n",
    "\n",
    "    # Drop the temporary 'seconds_since_midnight' column if you don't need it\n",
    "    new_df.drop('seconds_since_midnight', axis=1, inplace=True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def full_data_loader(location,n_lagged_features=3,threshold=0.9,normalization = 'minmax',date_encoding=True,verbose=True):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Load, preprocess, and prepare data for training and prediction including feature engineering, normalization, and resampling.\n",
    "\n",
    "    This function reads datasets, selects usable data, adds date encoding features, lagged features, and normalizes the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - `location` (str): The path or URL to the data files.\n",
    "    - `n_lagged_features` (int): The number of lagged features to add.\n",
    "    - `threshold` (float): The threshold for feature correlation identification.\n",
    "    - `normalization` (str): The type of data normalization ('minmax' or 'mean'), or 'none' for no normalization.\n",
    "    - `date_encoding` (bool): Whether to add date encoding features.\n",
    "    - `verbose` (bool): Whether to display progress and information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - Tuple of DataFrames, Series, and List: A tuple containing X_Train, Y_Train, X_Test, and a list of selected features.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```python\n",
    "    location = \"data/location\"\n",
    "    n_lagged_features = 3\n",
    "    threshold = 0.9\n",
    "    normalization = 'minmax'\n",
    "    date_encoding = True\n",
    "    verbose = True\n",
    "\n",
    "    X_Train, Y_Train, X_Test, selected_features = full_data_loader(\n",
    "        location, n_lagged_features, threshold, normalization, date_encoding, verbose\n",
    "    )\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Read datasets\n",
    "    if verbose: print(\"Reading datasets\")\n",
    "    train_target, X_train_observed, X_train_estimated, X_test_estimated, FEATURES = load_clean_datasets(location, threshold,verbose=False)\n",
    "    \n",
    "    selected_features = FEATURES[-1][1:] # all low correlation features from X_test_estimated except date\n",
    "\n",
    "    # Select usable data and resample training target\n",
    "    if verbose: print(\"Selecting training data\")\n",
    "    Ydf = remove_consecutive_identical_values(train_target,'pv_measurement')\n",
    "    Xdf = pd.concat([X_train_observed,X_train_estimated]).reset_index(drop=True)\n",
    "    X_Train,Y_Train = select_and_resample(Xdf,Ydf)\n",
    "    \n",
    "    # Add features to encode the date information\n",
    "    if date_encoding:\n",
    "        if verbose: print(\"Encoding date information\")\n",
    "        X_Train = convert_datetime(X_Train,'date_forecast')\n",
    "        X_Test = convert_datetime(X_test_estimated, 'date_forecast')\n",
    "        selected_features += ['sin_day']\n",
    "    else:\n",
    "        X_Test = X_test_estimated\n",
    "    \n",
    "    # Add lagged features\n",
    "    if n_lagged_features!=0:\n",
    "        if verbose: print(\"Adding lag features\")\n",
    "        time_lags = [15*n for n in range(1,n_lagged_features+1)] # in minutes\n",
    "        X_Train = lagged_features_FUTURE(X_Train,time_intervals=time_lags,verbose=False)\n",
    "        X_Test = lagged_features_FUTURE(X_Test,time_intervals=time_lags,verbose=False)\n",
    "        feature_list = selected_features\n",
    "        suffix_list = ['']+[f\"__p{lag}\" for lag in time_lags]\n",
    "        selected_features = [f+suffix for f in feature_list for suffix in suffix_list]\n",
    "\n",
    "    if date_encoding:\n",
    "        selected_features += ['sin_year','cos_year','forecast_interval']\n",
    "\n",
    "    # Normalize datasets\n",
    "    if normalization in ['minmax', 'mean']:\n",
    "        if verbose:print(\"Normalizing with \"+normalization)\n",
    "        if normalization == 'minmax':\n",
    "            # X_Train_normal = normalize_dataframe_minmax(X_Train)\n",
    "            X_Train_normal,X_Test_normal = normalize_dataframe_minmax_TEST(X_Train,X_Test)\n",
    "        if normalization == 'mean':\n",
    "            X_Train_normal = normalize_dataframe_mean_std(X_Train)\n",
    "            X_Test_normal = normalize_dataframe_mean_std_TEST(X_Train,X_Test)\n",
    "        X_Train = X_Train_normal\n",
    "        X_Test = X_Test_normal\n",
    "\n",
    "\n",
    "\n",
    "    if verbose :print(\"\\nDone\\n\")\n",
    "    return (X_Train,Y_Train['pv_measurement'],X_Test,selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the stacked model\n",
    "\n",
    "For level 0 we will use CatBoost, K-neighbors and DecisionTree. The optimal parameters for each model are searched with RandomSearchCV (see long notebook). \n",
    "\n",
    "For level 1 we will use a LightGBM model that takes as input the predictions of each model as well as the corresponding inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for all locations\n",
    "\n",
    "This code will save the trained model for each location as a `.joblib` file in the current directory that is later loaded to make the final predictions.\n",
    "\n",
    "**Runtime information :** on a simple 4 cores CPU laptop this took about 30 minutes of training per location, so it should be much faster on a more powerful machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOCATION A\n",
      "128 features selected\n",
      "\n",
      "77828 points in the training dataset\n",
      "Training in progress\n",
      "Training for A finished in 60.7 minutes\n",
      "\n",
      "LOCATION B\n",
      "118 features selected\n",
      "\n",
      "53596 points in the training dataset\n",
      "Training in progress\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training for B finished in 52.6 minutes\n",
      "\n",
      "LOCATION C\n",
      "143 features selected\n",
      "\n",
      "42360 points in the training dataset\n",
      "Training in progress\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training for C finished in 53.9 minutes\n"
     ]
    }
   ],
   "source": [
    "n_lagged_features = 4\n",
    "threshold = 0.9\n",
    "\n",
    "\n",
    "for LOCATION in ['A','B','C']:\n",
    "    print(f\"\\nLOCATION {LOCATION}\")\n",
    "\n",
    "    # Load seleted and usable data\n",
    "    X_Train_full,Y_Train_full,X_Test_full,selected_features = full_data_loader(location = LOCATION,\n",
    "                                                        n_lagged_features=n_lagged_features,\n",
    "                                                        threshold=threshold,\n",
    "                                                        normalization = 'minmax',\n",
    "                                                        date_encoding=True,\n",
    "                                                        verbose=False)\n",
    "    \n",
    "    print(f\"{len(selected_features)} features selected\\n\")\n",
    "    \n",
    "    # Select the correct columns for the prediction\n",
    "    X_Train = X_Train_full[selected_features].astype('float64')\n",
    "    X_Test = X_Test_full[selected_features].astype('float64')\n",
    "    Y_Train = Y_Train_full.astype('float64')\n",
    "\n",
    "    print(f\"{X_Train.shape[0]} points in the training dataset\")\n",
    "    \n",
    "    print(\"Training in progress\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Initialize the model base learners\n",
    "    level0 = list()\n",
    "    level0.append(('catboost', CatBoostRegressor(iterations=2000, # 1200\n",
    "                            depth=9,\n",
    "                            learning_rate=0.35,\n",
    "                            l2_leaf_reg=6,\n",
    "                            loss_function='RMSE',\n",
    "                            # task_type='GPU',\n",
    "                            verbose=0,\n",
    "                            allow_writing_files=False)))\n",
    "    level0.append(('decision_tree', DecisionTreeRegressor(splitter='random',\n",
    "                                                        min_samples_split=3,\n",
    "                                                        min_samples_leaf=1,\n",
    "                                                        max_depth=None,\n",
    "                                                        random_state=RANDOM_SEED)))\n",
    "    level0.append(('k_nearest_neighbor', KNeighborsRegressor(n_neighbors=4,\n",
    "                                                             algorithm='brute'))) \n",
    "    # define meta learner model\n",
    "    level1 = lgb.LGBMRegressor(tree_learner= 'serial',\n",
    "                            num_threads= -1,\n",
    "                            num_leaves= 500,\n",
    "                            n_estimators= 1200, \n",
    "                            metric= 'mse',\n",
    "                            learning_rate= 0.08,   \n",
    "                            reg_alpha=4,\n",
    "                            reg_lambda=8,\n",
    "                            verbose=0)\n",
    "\n",
    "    # define the stacking ensemble\n",
    "    model = StackingRegressor(estimators=level0,\n",
    "                            final_estimator=level1,\n",
    "                            passthrough=True, \n",
    "                            cv=10,\n",
    "                            n_jobs=-1,\n",
    "                            verbose = 0)\n",
    "\n",
    "    # fit the model on all available data\n",
    "    model.fit(X_Train, Y_Train)\n",
    "\n",
    "    # Save model\n",
    "    from joblib import dump\n",
    "    name = 'catboost_kneighbor_dectree_to_lgb_lag4_minmax_FUTURE_v3_feat0_'\n",
    "    dump(model, 'Stacked_models/'+name + LOCATION+'.joblib')\n",
    "    print(f\"Training for {LOCATION} finished in {(time.time() - start)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction for Kaggle\n",
    "\n",
    "This loop loads the saved models and makes a prediction in the `.csv` format expected for Kaggle submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS = []\n",
    "\n",
    "n_lagged_features = 4\n",
    "threshold = 0.9\n",
    "\n",
    "model_name = 'catboost_kneighbor_dectree_to_lgb_lag4_minmax_FUTURE_v4_feat0_'\n",
    "\n",
    "for LOCATION in ['A', 'B', 'C']:\n",
    "    \n",
    "    print(f\"\\nLOCATION {LOCATION}\")\n",
    "\n",
    "    model = load(model_name + LOCATION +'.joblib')\n",
    "    # Load selected and usable data\n",
    "    X_Train_full,Y_Train_full,X_Test_full,selected_features = full_data_loader(location = LOCATION,\n",
    "                                                        n_lagged_features=n_lagged_features,\n",
    "                                                        threshold=threshold,\n",
    "                                                        normalization = 'minmax',\n",
    "                                                        date_encoding=True,\n",
    "                                                        verbose=False)   \n",
    "    X_Test = X_Test_full[selected_features].astype('float64')\n",
    "\n",
    "    # Predict on the X_test_estimated dataset\n",
    "    Y_test_estimated = pd.DataFrame()\n",
    "\n",
    "    # Make predictions every 15 minutes\n",
    "    pv_measurement =  model.predict(X_Test)\n",
    "    pv_measurement[pv_measurement<0]=0\n",
    "    Y_test_estimated['prediction'] = pv_measurement\n",
    "\n",
    "    # Select only the predictions at round hours\n",
    "    Y_test_estimated['time'] = X_Test_full['date_forecast'].reset_index(drop=True)\n",
    "    Y_test_estimated_round = select_o_clock(Y_test_estimated,'time')\n",
    "    PREDICTIONS.append(Y_test_estimated_round['prediction'])\n",
    "\n",
    "# Concatenate all 3 predictions\n",
    "submission = pd.concat(PREDICTIONS).reset_index()\n",
    "# Add id column\n",
    "submission['id'] = submission.index\n",
    "\n",
    "# SAVE TO CSV \n",
    "name = \"GROUP17_catboost_kneighbor_dectree_to_lgb_lag4_minmax_FUTURE_v4_feat0_\"\n",
    "submission.to_csv(\"Submissions/\"+name+\".csv\",\n",
    "                  columns=['id','prediction'],\n",
    "                  index=False,\n",
    "                  encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
